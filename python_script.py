# -*- coding: utf-8 -*-
"""Backend - Sentiment Prediction from chat history.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/105Ml6837LoP3RQ-ZC_pnn1NKZlmslU7F
"""

# !pip install nlp
# !pip install tensorflow

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import nlp
import random


def show_history(h):
    plt.style.use('dark_background')
    epochs_trained = len(h.history['loss'])
    plt.figure(figsize=(16, 6))

    plt.subplot(1, 2, 1)
    plt.plot(range(0, epochs_trained), h.history.get('accuracy'), label='Training')
    plt.plot(range(0, epochs_trained), h.history.get('val_accuracy'), label='Validation')
    plt.ylim([0., 1.])
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(range(0, epochs_trained), h.history.get('loss'), label='Training')
    plt.plot(range(0, epochs_trained), h.history.get('val_loss'), label='Validation')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    
def show_confusion_matrix(y_true, y_pred, classes):
    from sklearn.metrics import confusion_matrix
    
    cm = confusion_matrix(y_true, y_pred, normalize='true')

    plt.figure(figsize=(8, 8))
    sp = plt.subplot(1, 1, 1)
    ctx = sp.matshow(cm)
    plt.xticks(list(range(0, 6)), labels=classes)
    plt.yticks(list(range(0, 6)), labels=classes)
    plt.colorbar(ctx)
    plt.show()

dataset =  nlp.load_dataset('emotion')

train = dataset['train']
val = dataset['validation']
test = dataset['test']

def get_tweet(data):
  tweets = [x['text']for x in data]
  labels = [x['label']for x in data]
  return tweets, labels

tweets,labels = get_tweet(train)

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=10000, oov_token ='<UNK>')
tokenizer.fit_on_texts(tweets)

tokenizer.texts_to_sequences([tweets[0]])

plt.hist(labels,bins =11)
plt.show()

maxlen = 50
from tensorflow.keras.preprocessing.sequence import pad_sequences
def get_sequences(tokenizer,tweets):
  sequence = tokenizer.texts_to_sequences(tweets)
  padded = pad_sequences(sequence, truncating = 'post', padding = 'post', maxlen = maxlen)
  return padded
padded_train_seq = get_sequences(tokenizer,tweets)

classes = set(labels)

class_to_index = dict((c,i) for i,c in enumerate(classes))
index_to_class = dict((v,k) for k,v in class_to_index.items())

class_to_index

names_to_ids = lambda labels:np.array([class_to_index.get(x) for x in labels])
train_labels = names_to_ids(labels)

model = tf.keras.models.Sequential([
        tf.keras.layers.Embedding(10000,16,input_length=maxlen),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20,return_sequences=True)),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),
        tf.keras.layers.Dense(6,activation='softmax')
])

model.compile(
    loss = 'sparse_categorical_crossentropy',
    optimizer = 'adam',
    metrics = ['accuracy']
)

model.summary()

val_tweets, val_labels = get_tweet(val)
val_seq = get_sequences(tokenizer,val_tweets)
val_labels = names_to_ids(val_labels)

h = model.fit(
    padded_train_seq, train_labels,
    validation_data = (val_seq,val_labels),
    epochs = 20,
    callbacks=[
               tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=2)
    ]
)

show_history(h)

model.save("mpc-model")

import pickle
with open('tokenizer.pickle','wb') as handle:
  pickle.dump(tokenizer,handle,protocol = pickle.HIGHEST_PROTOCOL)

with open('label_encoder.pickle','wb') as ecn_file:
  pickle.dump(index_to_class,ecn_file,protocol = pickle.HIGHEST_PROTOCOL)

test_sentences = ['Today is a wonderful day','My friend betray me!','i made that make me feel dumb and dumber','Tomorrow is exam','i feel soo lonely']
test_label = ['joy','anger','sadness','fear','sadness']
test_sequ = get_sequences(tokenizer,test_sentences)
test_label = names_to_ids(test_label)

for i in range (5):
  print('Sequence:',test_sentences[i])
  print('Emotion:',index_to_class[test_label[i]])

  p = model.predict(np.expand_dims(test_sequ[i],axis=0))[0]
  pred_class = index_to_class[np.argmax(p).astype('uint8')]

  print('Predicted Emotion:', pred_class,'\n')

import numpy as np
from tensorflow import keras
from sklearn.preprocessing import LabelEncoder
import pickle
from tensorflow.keras.preprocessing.sequence import pad_sequences

def load_model():
    model = keras.models.load_model('mpc-model')

    with open('tokenizer.pickle','rb') as handle:
        tokenizer = pickle.load(handle)
    with open('label_encoder.pickle','rb') as enc:
        index_to_class = pickle.load(enc)



def get_sequences(tokenizer,tweets):
  maxlen = 50
  sequence = tokenizer.texts_to_sequences(tweets)
  padded = pad_sequences(sequence, truncating = 'post', padding = 'post', maxlen = maxlen)
  return padded

def predicting(inp):
    lists = [inp]
    padded = get_sequences(tokenizer,lists)

    result = model.predict(np.expand_dims(padded[0],axis=0))[0]

    tag = index_to_class[np.argmax(result).astype('uint8')]   
    return tag

load_model()
print(predicting("I am good boy"))

txt = ""
while(txt != "exit"):
  txt = input()
  print(predicting(txt))


if _name_ == '_main_':
    globals()[sys.argv[1]](sys.argv[2])